(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{267:function(t,s,a){t.exports=a.p+"assets/img/t1.2813f6fc.png"},268:function(t,s,a){t.exports=a.p+"assets/img/t2.6411e863.png"},269:function(t,s,a){t.exports=a.p+"assets/img/t3.42b309a7.png"},341:function(t,s,a){"use strict";a.r(s);var n=a(28),p=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"t分布"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#t分布"}},[t._v("#")]),t._v(" t分布")]),t._v(" "),n("p",[t._v("t-分布（t-distribution）用于根据小样本来估计呈正态分布且方差未知的总体的均值。如果总体方差已知（例如在样本数量足够多时），则应该用正态分布来估计总体均值。")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 独立t分布检验")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检验两组数据的均值差异水平")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scipy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ss\n\n\nxx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ttest_ind"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rvs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("ss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rvs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Ttest_indResult(statistic=0.48605562136576796, pvalue=0.6307101254445986)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pvalue > 0.05  （以0.05作为显著性水平来比较的话，是可以接受假设的，即两组数的均值是没有差别的）")]),t._v("\n")])])]),n("h1",{attrs:{id:"交叉分析"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#交叉分析"}},[t._v("#")]),t._v(" 交叉分析")]),t._v(" "),n("h2",{attrs:{id:"交叉分析-二维图"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#交叉分析-二维图"}},[t._v("#")]),t._v(" 交叉分析.二维图")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 交叉分析")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 题目：分析各部门的离职率【left】之间是否有明显差异")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用的方法：独立t检验")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 思路，得到各部门的离职分布，然后两两间求t检验量，并求出p值")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scipy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ss\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" seaborn "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sns\n\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data/data/HR.csv'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 去掉异常值数据")]),t._v("\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"last_evaluation"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"salary"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"nme"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"department"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sale"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(df)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为了得到各部门的离职率分布，得先groupby一下")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# indices属性，获取索引")]),t._v("\ndp_indices "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("by"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"department"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("indices\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(dp_indices)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#'sales': array([    0,     1,     2, ..., 14969, 14970, 14971], dtype=int64), 'support': array([   46,    47,    48, ..., 14997, 14998, 14999], dtype=int64), 'technical': array([   35,    36,    37, ..., 14987, 14988, 14989], dtype=int64)} ")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解读：sales的项在dp中是第14969, 14970, 14971项")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 单个方式")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据索引获取值")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# sales_values = df["left"].iloc[dp_indices["sales"]].values')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(sales_values)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# technical_values = df["left"].iloc[dp_indices["technical"]].values')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(technical_values)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(ss.ttest_ind(sales_values,technical_values)[1])")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.2")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 推荐方式")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 两两比较")]),t._v("\ndp_keys "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_indices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(dp_keys)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dict_keys(['IT', 'RandD', 'accounting', 'hr', 'management', 'marketing', 'product_mng', 'sales', 'support', 'technical'])")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 弄个二维图来展示两两关系")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 声明一个二维矩阵")]),t._v("\ndp_t_mat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(dp_t_mat)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for i in range(len(dp_keys)):")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     for j in range(len(dp_keys)):")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#         p_value = ss.ttest_ind(df["left"].iloc[dp_indices[dp_keys[i]]].values,\\')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#             df["left"].iloc[dp_indices[dp_keys[j]]].values)[1]')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         dp_t_mat[i][j] = p_value")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.2")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" j "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        p_value "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ttest_ind"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"left"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dp_indices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n            df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"left"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dp_indices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" p_value "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            dp_t_mat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            dp_t_mat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" p_value\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(dp_t_mat)")]),t._v("\nsns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heatmap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dp_t_mat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("xticklabels"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("yticklabels"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dp_keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),n("p",[n("img",{attrs:{src:a(267),alt:"2.1"}}),t._v(" "),n("img",{attrs:{src:a(268),alt:"2.2"}})]),t._v(" "),n("h2",{attrs:{id:"交叉分析-透视表"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#交叉分析-透视表"}},[t._v("#")]),t._v(" 交叉分析.透视表")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scipy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ss\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" seaborn "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sns\n\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data/data/HR.csv'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 去掉异常值数据")]),t._v("\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"last_evaluation"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"salary"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"nme"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"department"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sale"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(df)")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 透视表")]),t._v("\npiv_tb "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pivot_table"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"left"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"promotion_last_5years"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"salary"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n    columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Work_accident"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("aggfunc"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(piv_tb)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Work_accident                        0         1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# promotion_last_5years salary")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 0                     high    0.082996  0.000000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                       low     0.331835  0.090020")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                       medium  0.230683  0.081655")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1                     high    0.000000  0.000000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                       low     0.229167  0.166667")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                       medium  0.028986  0.023256")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解读")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 0.331835  没有工作事故，低薪，离职最多")]),t._v("\n\nsns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heatmap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("piv_tb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("vmin"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("vmax"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cmap"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("sns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color_palette"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Reds"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_colors"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("img",{attrs:{src:a(269),alt:"透视图"}})])])}),[],!1,null,null,null);s.default=p.exports}}]);