(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{294:function(t,s,a){t.exports=a.p+"assets/img/sk1.287b7bef.png"},295:function(t,s){t.exports="data:image/png;base64,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"},296:function(t,s,a){t.exports=a.p+"assets/img/sk3.142c344f.svg"},297:function(t,s,a){t.exports=a.p+"assets/img/sse1.0cb1657d.jpg"},298:function(t,s,a){t.exports=a.p+"assets/img/Scikit-learn.8fe704ba.png"},299:function(t,s,a){t.exports=a.p+"assets/img/sk4.ef50699b.jpg"},300:function(t,s,a){t.exports=a.p+"assets/img/sk6.a5a87a5a.jpg"},301:function(t,s,a){t.exports=a.p+"assets/img/sk7.c762fb34.jpg"},302:function(t,s,a){t.exports=a.p+"assets/img/sklg.c484a042.jpg"},303:function(t,s,a){t.exports=a.p+"assets/img/sk10.e314ece6.jpg"},354:function(t,s,a){"use strict";a.r(s);var n=a(28),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"机器学习-入门"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#机器学习-入门"}},[t._v("#")]),t._v(" 机器学习 入门")]),t._v(" "),n("p"),n("div",{staticClass:"table-of-contents"},[n("ul",[n("li",[n("a",{attrs:{href:"#机器学习-入门"}},[t._v("机器学习 入门")]),n("ul",[n("li",[n("a",{attrs:{href:"#步骤"}},[t._v("步骤")])]),n("li",[n("a",{attrs:{href:"#特征、标签"}},[t._v("特征、标签")])]),n("li",[n("a",{attrs:{href:"#python机器学习包"}},[t._v("python机器学习包")])]),n("li",[n("a",{attrs:{href:"#协方差"}},[t._v("协方差")])]),n("li",[n("a",{attrs:{href:"#相关系数"}},[t._v("相关系数")]),n("ul",[n("li",[n("a",{attrs:{href:"#缺点"}},[t._v("缺点")])]),n("li",[n("a",{attrs:{href:"#相关系数矩阵"}},[t._v("相关系数矩阵")])])])]),n("li",[n("a",{attrs:{href:"#简单线性回归"}},[t._v("简单线性回归")]),n("ul",[n("li",[n("a",{attrs:{href:"#特征"}},[t._v("特征")])]),n("li",[n("a",{attrs:{href:"#标签"}},[t._v("标签")])]),n("li",[n("a",{attrs:{href:"#train-test-split"}},[t._v("traintestsplit")])]),n("li",[n("a",{attrs:{href:"#reshape-1-1"}},[t._v("reshape(-1,1)")])]),n("li",[n("a",{attrs:{href:"#r平方"}},[t._v("R平方")])]),n("li",[n("a",{attrs:{href:"#学习时常与分数"}},[t._v("学习时常与分数")])]),n("li",[n("a",{attrs:{href:"#学习时常与是否通过考试-逻辑回归"}},[t._v("学习时常与是否通过考试(逻辑回归)")])])])]),n("li",[n("a",{attrs:{href:"#相关关系与因果关系"}},[t._v("相关关系与因果关系")])]),n("li",[n("a",{attrs:{href:"#逻辑预测"}},[t._v("逻辑预测")]),n("ul",[n("li",[n("a",{attrs:{href:"#code文件里有代码示例"}},[t._v("code文件里有代码示例")])])])]),n("li",[n("a",{attrs:{href:"#我机器学习的第一个案例代码"}},[t._v("我机器学习的第一个案例代码")]),n("ul",[n("li",[n("a",{attrs:{href:"#步骤"}},[t._v("步骤")])]),n("li",[n("a",{attrs:{href:"#数据类型缺失值处理"}},[t._v("数据类型缺失值处理")])]),n("li",[n("a",{attrs:{href:"#映射，类似emun类型-小技巧1"}},[t._v("映射，类似emun类型,小技巧1")])]),n("li",[n("a",{attrs:{href:"#one-hot编码"}},[t._v("one-hot编码")])]),n("li",[n("a",{attrs:{href:"#concat将生成的one-hot数据与原数据连接起来"}},[t._v("concat将生成的one-hot数据与原数据连接起来")])]),n("li",[n("a",{attrs:{href:"#相关系数法：计算各个特征的相关系数"}},[t._v("相关系数法：计算各个特征的相关系数")])]),n("li",[n("a",{attrs:{href:"#泰坦尼克号生存率预测"}},[t._v("泰坦尼克号生存率预测")])]),n("li",[n("a",{attrs:{href:"#步骤"}},[t._v("步骤")])]),n("li",[n("a",{attrs:{href:"#数据拆分"}},[t._v("数据拆分")])]),n("li",[n("a",{attrs:{href:"#泰坦尼克号案例的补充"}},[t._v("泰坦尼克号案例的补充")])])])]),n("li",[n("a",{attrs:{href:"#结合taitan-test-py看"}},[t._v("结合taitan-test.py看")])])])])])]),n("p"),t._v(" "),n("h2",{attrs:{id:"步骤"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#步骤"}},[t._v("#")]),t._v(" 步骤")]),t._v(" "),n("ol",[n("li",[t._v("提出问题")]),t._v(" "),n("li",[t._v("理解数据")]),t._v(" "),n("li",[t._v("数据清洗")]),t._v(" "),n("li",[t._v("构建模型")]),t._v(" "),n("li",[t._v("评估")])]),t._v(" "),n("h2",{attrs:{id:"特征、标签"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#特征、标签"}},[t._v("#")]),t._v(" 特征、标签")]),t._v(" "),n("p",[t._v("输入：特征   输出：标签")]),t._v(" "),n("h2",{attrs:{id:"python机器学习包"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#python机器学习包"}},[t._v("#")]),t._v(" python机器学习包")]),t._v(" "),n("p",[t._v("scikit-learn")]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("机器学习常常用来解决相关性分析的问题")]),t._v(" "),n("h2",{attrs:{id:"协方差"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#协方差"}},[t._v("#")]),t._v(" 协方差")]),t._v(" "),n("p",[n("img",{attrs:{src:a(294),alt:""}})]),t._v(" "),n("p",[t._v("标准差描述的就是这种“散布度”")]),t._v(" "),n("p",[t._v("标准差和方差一般是用来描述一维数据的\n"),n("img",{attrs:{src:a(295),alt:""}}),t._v("\n协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况.\n从直观上来看，协方差表示的是两个变量总体误差的期望。\n如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值时另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值；如果两个变量的变化趋势相反，即其中一个变量大于自身的期望值时另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。")]),t._v(" "),n("h2",{attrs:{id:"相关系数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#相关系数"}},[t._v("#")]),t._v(" 相关系数")]),t._v(" "),n("p",[t._v("研究变量之间线性相关程度的量")]),t._v(" "),n("p",[t._v("简单相关系数：又叫相关系数或线性相关系数，一般用字母r表示，用来度量两个变量间的线性关系。\n"),n("img",{attrs:{src:a(296),alt:""}}),t._v("\n标准差（西格玛）")]),t._v(" "),n("p",[t._v("性值：\n|r(x,y)| <= 1   ,值越大，相关程度越大，1为正相关，-1为负相关，0为相关程度最低")]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",[t._v("r的值")]),t._v(" "),n("th",{staticStyle:{"text-align":"center"}},[t._v("描述")])])]),t._v(" "),n("tbody",[n("tr",[n("td",[t._v("1 ~ 0.6")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("强相关")])]),t._v(" "),n("tr",[n("td",[t._v("0.6 ~ 0.3")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("中等程度相关")])]),t._v(" "),n("tr",[n("td",[t._v("0.3 ~ 0")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("若相关")])])])]),t._v(" "),n("h3",{attrs:{id:"缺点"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#缺点"}},[t._v("#")]),t._v(" 缺点")]),t._v(" "),n("p",[t._v("需要指出的是，相关系数有一个明显的缺点，即它接近于1的程度与数据组数n相关，这容易给人一种假象。因为，当n较小时，相关系数的波动较大，对有些样本相关系数的绝对值易接近于1；当n较大时，相关系数的绝对值容易偏小。特别是当n=2时，相关系数的绝对值总为1。因此在样本容量n较小时，我们仅凭相关系数较大就判定变量x与y之间有密切的线性关系是不妥当的。")]),t._v(" "),n("h3",{attrs:{id:"相关系数矩阵"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#相关系数矩阵"}},[t._v("#")]),t._v(" 相关系数矩阵")]),t._v(" "),n("p",[n("strong",[t._v("相关系数矩阵 = pd_dataFrame.corr()")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\nx1"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx2"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 利用pandas")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("x2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("x3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'数据:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'相关系数矩阵为：\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nr_ab"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nr_ac"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nr_bc"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nr_ab_c"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("r_ab"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("r_ac"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("r_bc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("r_ac"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("r_bc"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ab_c的一阶偏相关系数为：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("r_ab_c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#            a         b         c")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 0  0.021221  0.425165  0.971476")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1  0.313436  0.034512  0.601296")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2  0.586531  0.934413  0.523409")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3  0.532916  0.198744  0.127394")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4  0.712067  0.646639  0.716697")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5  0.342901  0.841756  0.118820")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6  0.298651  0.526323  0.435489")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 7  0.515808  0.322576  0.022877")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 8  0.520162  0.890742  0.750954")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 9  0.256005  0.752313  0.762069")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 相关系数矩阵为：")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#            a         b         c")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a  1.000000  0.211257 -0.344704")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# b  0.211257  1.000000  0.209762")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# c -0.344704  0.209762  1.000000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ab_c的一阶偏相关系数为： 0.3089499284601962")]),t._v("\n\n")])])]),n("h2",{attrs:{id:"简单线性回归"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#简单线性回归"}},[t._v("#")]),t._v(" 简单线性回归")]),t._v(" "),n("h3",{attrs:{id:"特征"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#特征"}},[t._v("#")]),t._v(" 特征")]),t._v(" "),n("p",[t._v("数据的属性，如橘子的颜色，产地，数据的特点")]),t._v(" "),n("h3",{attrs:{id:"标签"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#标签"}},[t._v("#")]),t._v(" 标签")]),t._v(" "),n("p",[t._v("数据的结果，如嫩黄色橘子是甜的")]),t._v(" "),n("p",[n("img",{attrs:{src:a(297),alt:""}}),t._v(" "),n("img",{attrs:{src:a(298),alt:""}})]),t._v(" "),n("h3",{attrs:{id:"train-test-split"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#train-test-split"}},[t._v("#")]),t._v(" train_test_split")]),t._v(" "),n("p",[t._v("从样本中随机的按比例选取 训练数据 和 测试数据")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("X_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X_test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("exam_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("exam_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("train_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v(".8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("X_train :训练数据 标签\nX_test\t:测试数据标签")]),t._v(" "),n("p",[t._v("y_train\t:训练数据特征\ny_test\t:测试数据特征\n"),n("img",{attrs:{src:a(299),alt:""}})]),t._v(" "),n("h3",{attrs:{id:"reshape-1-1"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#reshape-1-1"}},[t._v("#")]),t._v(" reshape(-1,1)")]),t._v(" "),n("h3",{attrs:{id:"r平方"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#r平方"}},[t._v("#")]),t._v(" R平方")]),t._v(" "),n("p",[n("strong",[t._v("0<=R平方<=1")]),t._v("\n决定系数R平方，功能")]),t._v(" "),n("ol",[n("li",[t._v("回归线拟合程度————有多少百分比的y波动可以有回归线来描述（x的波动变化）")]),t._v(" "),n("li",[t._v("值大小————R平方越高，回归模型越精确")])]),t._v(" "),n("p",[t._v("r平方 = 0，说明无法从x值预测y值\nr平方 = 1，说明可以从x值预测y值，且没有误差\n"),n("img",{attrs:{src:a(300),alt:""}}),t._v(" "),n("img",{attrs:{src:a(301),alt:""}})]),t._v(" "),n("h3",{attrs:{id:"学习时常与分数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#学习时常与分数"}},[t._v("#")]),t._v(" 学习时常与分数")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OrderedDict\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#数据集")]),t._v("\nexamDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'学习时间'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'分数'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("22")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("43")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("22")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("33")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("62")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n              "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("48")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("62")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("73")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("81")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("76")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("82")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("90")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("93")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nexamOrderDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("OrderedDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("examDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nexamDf"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("examOrderDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #提取特征和标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #特征features")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# exam_X=examDf.loc[:,'学习时间']")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #标签labes")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# exam_y=examDf.loc[:,'分数']")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #绘制散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import matplotlib.pyplot as plt")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(exam_X, exam_y, color="b", label="exam data")')]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #添加图标标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.xlabel("Hours")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.ylabel("Score")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #显示图像")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.show()")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #相关系数：corr返回结果是一个数据框，存放的是相关系数矩阵")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# rDf=examDf.corr()")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('相关系数矩阵：')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(rDf)")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 线性回归的实现")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.提取特征和标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#特征features")]),t._v("\nexam_X"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("examDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'学习时间'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#标签labes")]),t._v("\nexam_y"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("examDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'分数'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.建立训练数据和测试数据")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\ntrain_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取训练数据（train）和测试数据（test）\n第一个参数：所要划分的样本特征\n第2个参数：所要划分的样本标签\ntrain_size：训练数据占比，如果是整数的话就是样本的数量\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nsklearn包0.8版本以后，需要将之前的sklearn.cross_validation 换成sklearn.model_selection\n所以课程中的代码\nfrom sklearn.cross_validation import train_test_split \n更新为下面的代码\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#建立训练数据和测试数据")]),t._v("\nX_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X_test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("exam_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                       exam_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                       train_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v(".8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #输出数据大小")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('原始数据特征：',exam_X.shape ,")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#       '，训练数据特征：', X_train.shape , ")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#       '，测试数据特征：',X_test.shape )")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('原始数据标签：',exam_y.shape ,")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#       '训练数据标签：', y_train.shape ,")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#       '测试数据标签：' ,y_test.shape)")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #绘制散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import matplotlib.pyplot as plt")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(X_train, y_train, color="blue", label="train data")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(X_test, y_test, color="red", label="test data")')]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #添加图标标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.legend(loc=2)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.xlabel("Hours")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.ylabel("Score")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #显示图像")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.show()")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# # 3.训练模型（使用训练数据）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# '''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 运行后会报错，因为这里输入的特征只有1个。注意看报错信息，通过这个例子也学会如何分析报错信息")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# '''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #第1步：导入线性回归")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# from sklearn.linear_model import LinearRegression")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# # 第2步：创建模型：线性回归")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model = LinearRegression()")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #第3步：训练模型")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# # model.fit(X_train , y_train)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n上面的报错内容，最后一行是这样提示我们的：\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n上面报错的内容翻译过来就是：\n如果你输入的数据只有1个特征，需要用array.reshape(-1, 1)来改变数组的形状\nshape是形状的意思，有首歌叫《shape of you》里面指的是女孩的身材令人难以忘记。在数据里就是指数据的大小。\nnumpy的reshape就是指改变数组的形状，下面通过几个案例你就明白了\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nreshape行的参数是-1表示什么呢？例如reshape(-1,列数)\n如果行的参数是-1，就会根据所给的列数，自动按照原始数组的大小形成一个新的数组，\n例如reshape(-1,1)就是改变成1列的数组，这个数组的长度是根据原始数组的大小来自动形成的。\n原始数组总共是2行*3列=6个数，那么这里就会形成6行*1列的数组\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n理解了reshape后，我们再来看下逻辑回归模型\nsklearn要求输入的特征必须是二维数组的类型，但是因为我们目前只有1个特征，所以需要用安装错误提示用reshape转行成二维数组的类型。\n错误提示信息：Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#将训练数据特征转换成二维数组XX行*1列")]),t._v("\nX_train"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#将测试数据特征转换成二维数组行数*1列")]),t._v("\nX_test"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第1步：导入线性回归")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LinearRegression\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 第2步：创建模型：线性回归")]),t._v("\nmodel "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LinearRegression"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第3步：训练模型")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n最佳拟合线：z=𝑎+𝑏x\n截距intercept：a\n回归系数：b\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#截距")]),t._v("\na"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("intercept_\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#回归系数")]),t._v("\nb"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("coef_\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'最佳拟合线：截距a='")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'，回归系数b='")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.模型评估（使用测试数据）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#线性回归的scroe方法得到的是决定系数R平方")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#评估模型:决定系数R平方")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nscore内部会对第一个参数X_test用拟合曲线自动计算出y预测值，内容是决定系数R平方的计算过程。所以我们只用根据他的要求输入参数即可。\n'''")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#导入绘图包")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n第1步：绘制训练数据散点图\n'''")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scatter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blue'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train data"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n第2步：用训练数据绘制最佳线\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#最佳拟合线训练数据的预测值")]),t._v("\ny_train_pred "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#绘制最佳拟合线：标签用的是训练数据的预测值y_train_pred")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train_pred"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'black'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" linewidth"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"best line"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n第3步：绘制测试数据的散点图\n'''")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scatter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'red'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test data"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加图标标签")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("legend"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hours"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Score"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#显示图像")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"学习时常与是否通过考试-逻辑回归"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#学习时常与是否通过考试-逻辑回归"}},[t._v("#")]),t._v(" 学习时常与是否通过考试(逻辑回归)")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OrderedDict\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#数据集")]),t._v("\nexamDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'学习时间'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.00")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'通过考试'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nexamOrderDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("OrderedDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("examDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nexamDf"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("examOrderDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#提取特征和标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#特征features")]),t._v("\nexam_X"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("examDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'学习时间'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#标签labes")]),t._v("\nexam_y"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("examDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'通过考试'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import matplotlib.pyplot as plt")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(exam_X, exam_y, color="b", label="exam data")')]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #添加图标标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.xlabel("Hours")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.ylabel("Pass")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #显示图像")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.show()")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 建立训练数据集和测试数据集")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\ntrain_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取训练数据（train）和测试数据（test）\n第一个参数：所要划分的样本特征\n第2个参数：所要划分的样本标签\ntrain_size：训练数据占比，如果是整数的话就是样本的数量\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nsklearn包0.8版本以后，需要将之前的sklearn.cross_validation 换成sklearn.model_selection\n所以课程中的代码\nfrom sklearn.cross_validation import train_test_split \n更新为下面的代码\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#建立训练数据和测试数据")]),t._v("\nX_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X_test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("exam_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                       exam_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                       train_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v(".8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                                                       "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#输出数据大小")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("exam_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'，训练数据特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'，测试数据特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据标签：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("exam_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'训练数据标签：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'测试数据标签：'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #绘制散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import matplotlib.pyplot as plt")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #散点图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(X_train, y_train, color="blue", label="train data")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.scatter(X_test, y_test, color="red", label="test data")')]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #添加图标标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.legend(loc=2)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.xlabel("Hours")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# plt.ylabel("Pass")')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #显示图像")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# plt.show()")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 训练模型（使用训练数据）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n上面的报错内容，最后一行是这样提示我们的：\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n上面报错的内容翻译过来就是：\n如果你输入的数据只有1个特征，需要用array.reshape(-1, 1)来改变数组的形状\nshape是形状的意思，有首歌叫《shape of you》里面指的是女孩的身材令人难以忘记。在数据里就是指数据的大小。\nnumpy的reshape就是指改变数组的形状，下面通过几个案例你就明白了\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nreshape列的参数是-1表示什么呢？例如reshape(行数,-1)\n如果列的参数是-1，就会根据所给的行数，自动按照原始数组的大小形成一个新的数组，\n例如reshape(1,-1)就是改变成1行的数组，这个数组的列数是根据原始数组的大小来自动形成的。\n原始数组总共是2行*3列=6个数，那么这里就会形成1行*6列的数组\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n理解了reshape后，我们再来看下逻辑回归模型\nsklearn要求输入的特征必须是二维数组的类型，但是因为我们目前只有1个特征，所以需要用安装错误提示用reshape转行成二维数组的类型。\n错误提示信息：Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#将训练数据特征转换成二维数组XX行*1列")]),t._v("\nX_train"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#将测试数据特征转换成二维数组行数*1列")]),t._v("\nX_test"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#导入逻辑回归包")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LogisticRegression\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建模型：逻辑回归")]),t._v("\nmodel "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LogisticRegression"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#训练模型")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型评估（使用测试数据）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#评估模型：准确率")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 进一步理解什么是逻辑函数")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#获取概率值")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第1个值是标签为0的概率值，第2个值是标签为1的概率值")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_proba"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_proba"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测数据：使用模型的predict方法可以进行预测。这里我们输入学生的特征学习时间3小时，模型返回结果标签是1，表示预测该学生通过考试。")]),t._v("\npred"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n理解逻辑回归函数\n斜率slope\n截距intercept\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第1步：得到回归方程的z值")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#回归方程：z=𝑎+𝑏x")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#截距")]),t._v("\na"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("intercept_\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#回归系数")]),t._v("\nb"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("coef_\n\nx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\nz"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("a"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("x\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第2步：将z值带入逻辑回归函数中，得到概率值")]),t._v("\ny_pred"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("z"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'预测的概率值：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y_pred"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"相关关系与因果关系"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#相关关系与因果关系"}},[t._v("#")]),t._v(" 相关关系与因果关系")]),t._v(" "),n("h2",{attrs:{id:"逻辑预测"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#逻辑预测"}},[t._v("#")]),t._v(" 逻辑预测")]),t._v(" "),n("h3",{attrs:{id:"code文件里有代码示例"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#code文件里有代码示例"}},[t._v("#")]),t._v(" code文件里有代码示例")]),t._v(" "),n("h2",{attrs:{id:"我机器学习的第一个案例代码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#我机器学习的第一个案例代码"}},[t._v("#")]),t._v(" 我机器学习的第一个案例代码")]),t._v(" "),n("h3",{attrs:{id:"步骤-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#步骤-2"}},[t._v("#")]),t._v(" 步骤")]),t._v(" "),n("ol",[n("li",[t._v("选择子集")]),t._v(" "),n("li",[t._v("列名重命名")]),t._v(" "),n("li",[t._v("缺失数据处理")]),t._v(" "),n("li",[t._v("数据类型转换")]),t._v(" "),n("li",[t._v("数据排序")]),t._v(" "),n("li",[t._v("异常值处理")])]),t._v(" "),n("h3",{attrs:{id:"数据类型缺失值处理"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#数据类型缺失值处理"}},[t._v("#")]),t._v(" 数据类型缺失值处理")]),t._v(" "),n("p",[t._v("用平均值填充")]),t._v(" "),n("h3",{attrs:{id:"映射，类似emun类型-小技巧1"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#映射，类似emun类型-小技巧1"}},[t._v("#")]),t._v(" 映射，类似emun类型,小技巧1")]),t._v(" "),n("p",[n("strong",[t._v("dataFrame.map(dict)")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n将性别的值映射为数值\n男（male）对应数值1，女（female）对应数值0\n'''")]),t._v("\nsex_mapDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'male'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'female'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#map函数：对Series每个数据应用自定义的函数计算")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sex_mapDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"one-hot编码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#one-hot编码"}},[t._v("#")]),t._v(" one-hot编码")]),t._v(" "),n("p",[n("strong",[t._v("使用get_dummies进行one-hot编码")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#客舱等级(Pclass):")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("等舱，"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("等舱，"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("等舱\n"),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("'\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放提取后的特征")]),t._v("\npclassDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#使用get_dummies进行one-hot编码，列名前缀是Pclass")]),t._v("\npclassDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prefix"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"concat将生成的one-hot数据与原数据连接起来"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#concat将生成的one-hot数据与原数据连接起来"}},[t._v("#")]),t._v(" concat将生成的one-hot数据与原数据连接起来")]),t._v(" "),n("p",[n("strong",[t._v("pd.concat([dataFrame1,dataFrame2],axis=1)")]),t._v("\naxis=1,横向连接")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("embarkedDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#删掉客舱等级（Pclass）这一列")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"相关系数法：计算各个特征的相关系数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#相关系数法：计算各个特征的相关系数"}},[t._v("#")]),t._v(" 相关系数法：计算各个特征的相关系数")]),t._v(" "),n("p",[t._v("根据各个特征与生成情况（Survived）的相关系数大小，我们选择了这几个特征作为模型的输入")]),t._v(" "),n("h3",{attrs:{id:"泰坦尼克号生存率预测"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#泰坦尼克号生存率预测"}},[t._v("#")]),t._v(" 泰坦尼克号生存率预测")]),t._v(" "),n("h3",{attrs:{id:"步骤-3"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#步骤-3"}},[t._v("#")]),t._v(" 步骤")]),t._v(" "),n("p",[n("img",{attrs:{src:a(302),alt:""}})]),t._v(" "),n("h3",{attrs:{id:"数据拆分"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#数据拆分"}},[t._v("#")]),t._v(" 数据拆分")]),t._v(" "),n("p",[n("img",{attrs:{src:a(303),alt:""}}),t._v("\ncode文件中有")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 目录 ")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 提出问题（Business Understanding ）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 什么样的人在泰坦尼克号中更容易存活？ ")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 理解数据（Data Understanding）")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 采集数据")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 导入数据")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查看数据集信息")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据清洗（Data Preparation ）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据预处理")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 特征工程（Feature Engineering）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建模型（Modeling）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型评估（Evaluation）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 方案实施 （Deployment）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提交结果到Kaggle")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 报告撰写")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 忽略警告提示")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" warnings\nwarnings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filterwarnings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ignore'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#导入处理数据包")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#导入数据")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#训练数据集")]),t._v("\ntrain "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./taitannike/train.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#测试数据集")]),t._v("\ntest  "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./taitannike/test.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#这里要记住训练数据集有891条数据，方便后面从中拆分出测试数据集用于提交Kaggle结果")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print ('训练数据集:',train.shape,'测试数据集:',test.shape)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#合并数据集，方便同时对两个数据集进行清洗")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" test "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ignore_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print ('合并后的数据集:',full.shape)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(full)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\ndescribe只能查看数据类型的描述统计信息，对于其他类型的数据不显示，比如字符串类型姓名（name），客舱号（Cabin）\n这很好理解，因为描述统计指标是计算数值，所以需要该列的数据类型是数据\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#获取数据类型列的描述统计信息")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(full.describe())")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查看每一列的数据类型，和数据总数")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# full.info()")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n我们发现数据总共有1309行。\n其中数据类型列：年龄（Age）、船舱号（Cabin）里面有缺失数据：\n1）年龄（Age）里面数据总数是1046条，缺失了1309-1046=263，缺失率263/1309=20%\n2）船票价格（Fare）里面数据总数是1308条，缺失了1条数据\n\n字符串列：\n1）登船港口（Embarked）里面数据总数是1307，只缺失了2条数据，缺失比较少\n2）船舱号（Cabin）里面数据总数是295，缺失了1309-295=1014，缺失率=1014/1309=77.5%，缺失比较大\n这为我们下一步数据清洗指明了方向，只有知道哪些数据缺失数据，我们才能有针对性的处理。\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\\n我们发现数据总共有1309行。\n\\n其中数据类型列：年龄（Age）、船舱号（Cabin）里面有缺失数据：\\n1）年龄（Age）里面数据总数是1046条，\n缺失了1309-1046=263，缺失率263/1309=20%\\n2）船票价格（Fare）里面数据总数是1308条，缺失了1条数据\\n\\n字符串列：\\n1）\n登船港口（Embarked）里面数据总数是1307，只缺失了2条数据，缺失比较少\\n2）船舱号（Cabin）里面数据总数是295，\n缺失了1309-295=1014，缺失率=1014/1309=77.5%，缺失比较大\\n这为我们下一步数据清洗指明了方向，只有知道哪些数据缺失数据，\n我们才能有针对性的处理。\\n'''")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.数据清洗（Data Preparation ）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.1 数据预处理")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 缺失值处理")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在前面，理解数据阶段，我们发现数据总共有1309行。 其中数据类型列：年龄（Age）、船舱号（Cabin）里面有缺失数据。 字符串列：登船港口（Embarked）、船舱号（Cabin）里面有缺失数据。")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这为我们下一步数据清洗指明了方向，只有知道哪些数据缺失数据，我们才能有针对性的处理。")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 很多机器学习算法为了训练模型，要求所传入的特征中不能有空值。")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是数值类型，用平均值取代")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是分类数据，用最常见的类别取代")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用模型预测缺失值，例如：K-NN")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n我们发现数据总共有1309行。\n其中数据类型列：年龄（Age）、船舱号（Cabin）里面有缺失数据：\n1）年龄（Age）里面数据总数是1046条，缺失了1309-1046=263，缺失率263/1309=20%\n2）船票价格（Fare）里面数据总数是1308条，缺失了1条数据\n\n对于数据类型，处理缺失值最简单的方法就是用平均数来填充缺失值\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('处理前：')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# full.info()")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 年龄(Age)")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Age'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Age'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Age'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 船票价格(Fare)")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Fare'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Fare'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Fare'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('处理红后：')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# full.info()")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n总数据是1309\n字符串列：\n1）登船港口（Embarked）里面数据总数是1307，只缺失了2条数据，缺失比较少\n2）船舱号（Cabin）里面数据总数是295，缺失了1309-295=1014，缺失率=1014/1309=77.5%，缺失比较大\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#登船港口（Embarked）：查看里面数据长啥样")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n出发地点：S=英国南安普顿Southampton\n途径地点1：C=法国 瑟堡市Cherbourg\n途径地点2：Q=爱尔兰 昆士敦Queenstown\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n分类变量Embarked，看下最常见的类别，用其填充\n'''")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n从结果来看，S类别最常见。我们将缺失值填充为最频繁出现的值：\nS=英国南安普顿Southampton\n'''")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'S'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#缺失数据比较多，船舱号（Cabin）缺失值填充为U，表示未知（Uknow） ")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'U'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.2 特征提取")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.2.1数据分类")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查看数据类型，分为3种数据类型。并对类别数据处理：用数值代替类别，并进行One-hot编码")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n1.数值类型：\n乘客编号（PassengerId），年龄（Age），船票价格（Fare），同代直系亲属人数（SibSp），不同代直系亲属人数（Parch）\n2.时间序列：无\n3.分类数据：\n1）有直接类别的\n乘客性别（Sex）：男性male，女性female\n登船港口（Embarked）：出发地点S=英国南安普顿Southampton，途径地点1：C=法国 瑟堡市Cherbourg，出发地点2：Q=爱尔兰 昆士敦Queenstown\n客舱等级（Pclass）：1=1等舱，2=2等舱，3=3等舱\n2）字符串类型：可能从这里面提取出特征来，也归到分类数据中\n乘客姓名（Name）\n客舱号（Cabin）\n船票编号（Ticket）\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.2.1 分类数据：有直接类别的")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 乘客性别（Sex）： 男性male，女性female")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 登船港口（Embarked）：出发地点S=英国南安普顿Southampton，途径地点1：C=法国 瑟堡市Cherbourg，出发地点2：Q=爱尔兰 昆士敦Queenstown")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 客舱等级（Pclass）：1=1等舱，2=2等舱，3=3等舱")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 性别")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#查看性别数据这一列")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n将性别的值映射为数值\n男（male）对应数值1，女（female）对应数值0\n'''")]),t._v("\nsex_mapDict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'male'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'female'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#map函数：对Series每个数据应用自定义的函数计算")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sex_mapDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 登船港口(Embarked)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放提取后的特征")]),t._v("\nembarkedDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n使用get_dummies进行one-hot编码，产生虚拟变量（dummy variables），列名前缀是Embarked\n'''")]),t._v("\nembarkedDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prefix"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nembarkedDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("embarkedDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n因为已经使用登船港口(Embarked)进行了one-hot编码产生了它的虚拟变量（dummy variables）\n所以这里把登船港口(Embarked)删掉\n'''")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Embarked'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n上面drop删除某一列代码解释：\n因为drop(name,axis=1)里面指定了name是哪一列，比如指定的是A这一列，axis=1表示按行操作。\n那么结合起来就是把A列里面每一行删除，最终结果是删除了A这一列.\n简单来说，使用drop删除某几列的方法记住这个语法就可以了：drop([列名1,列名2],axis=1)\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Age\tCabin\tFare\tName\tParch\tPassengerId\tPclass\tSex\tSibSp\tSurvived\tTicket\tEmbarked_C\tEmbarked_Q\tEmbarked_S")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 0\t22.0\tU\t7.2500\tBraund, Mr. Owen Harris\t0\t1\t3\t1\t1\t0.0\tA/5 21171\t0\t0\t1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1\t38.0\tC85\t71.2833\tCumings, Mrs. John Bradley (Florence Briggs Th...\t0\t2\t1\t0\t1\t1.0\tPC 17599\t1\t0\t0")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2\t26.0\tU\t7.9250\tHeikkinen, Miss. Laina\t0\t3\t3\t0\t0\t1.0\tSTON/O2. 3101282\t0\t0\t1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3\t35.0\tC123\t53.1000\tFutrelle, Mrs. Jacques Heath (Lily May Peel)\t0\t4\t1\t0\t1\t1.0\t113803\t0\t0\t1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4\t35.0\tU\t8.0500\tAllen, Mr. William Henry\t0\t5\t3\t1\t0\t0.0\t373450\t0\t0\t")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 客舱等级（Pclass）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n客舱等级(Pclass):\n1=1等舱，2=2等舱，3=3等舱\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放提取后的特征")]),t._v("\npclassDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#使用get_dummies进行one-hot编码，列名前缀是Pclass")]),t._v("\npclassDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prefix"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npclassDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("pclassDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#删掉客舱等级（Pclass）这一列")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pclass'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(full)")]),t._v("\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.2.1 分类数据：字符串类型")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 字符串类型：可能从这里面提取出特征来，也归到分类数据中，这里数据有：")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 乘客姓名（Name）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 客舱号（Cabin）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 船票编号（Ticket）")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从姓名中提取头衔")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n查看姓名这一列长啥样\n注意到在乘客名字（Name）中，有一个非常显著的特点：\n乘客头衔每个名字当中都包含了具体的称谓或者说是头衔，将这部分信息提取出来后可以作为非常有用一个新变量，可以帮助我们进行预测。\n例如：\nBraund, Mr. Owen Harris\nHeikkinen, Miss. Laina\nOliva y Ocana, Dona. Fermina\nPeter, Master. Michael J\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n定义函数：从姓名中获取头衔\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getTitle")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    str1"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Mr. Owen Harris")]),t._v("\n    str2"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("str1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Mr")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#strip() 方法用于移除字符串头尾指定的字符（默认为空格）")]),t._v("\n    str3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("str2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" str3\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放提取后的特征")]),t._v("\ntitleDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#map函数：对Series每个数据应用自定义的函数计算")]),t._v("\ntitleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Title'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("getTitle"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n定义以下几种头衔类别：\nOfficer政府官员\nRoyalty王室（皇室）\nMr已婚男士\nMrs已婚妇女\nMiss年轻未婚女子\nMaster有技能的人/教师\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#姓名中头衔字符串与定义头衔类别的映射关系")]),t._v("\ntitle_mapDict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Capt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Officer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Col"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Officer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Major"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Officer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Jonkheer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Don"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Sir"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Dr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("         "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Officer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Rev"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Officer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"the Countess"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Dona"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mme"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mrs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mlle"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Miss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Ms"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("         "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mrs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mr"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mrs"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Mrs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Miss"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Miss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Master"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Master"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Lady"')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Royalty"')]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#map函数：对Series每个数据应用自定义的函数计算")]),t._v("\ntitleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Title'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" titleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Title'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("title_mapDict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#使用get_dummies进行one-hot编码")]),t._v("\ntitleDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("titleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Title'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("titleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#删掉姓名这一列")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#从客舱号中提取客舱类别")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放客舱号信息")]),t._v("\ncabinDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n客场号的类别值是首字母，例如：\nC85 类别映射为首字母C\n'''")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##使用get_dummies进行one-hot编码，列名前缀是Cabin")]),t._v("\ncabinDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_dummies"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prefix "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cabinDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#删掉客舱号这一列")]),t._v("\nfull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cabin'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 建立家庭人数和家庭类别")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#存放家庭信息")]),t._v("\nfamilyDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n家庭人数=同代直系亲属数（Parch）+不同代直系亲属数（SibSp）+乘客自己\n（因为乘客自己也是家庭成员的一个，所以这里加1）\n'''")]),t._v("\nfamilyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'FamilySize'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Parch'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SibSp'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n家庭类别：\n小家庭Family_Single：家庭人数=1\n中等家庭Family_Small: 2<=家庭人数<=4\n大家庭Family_Large: 家庭人数>=5\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#if 条件为真的时候返回if前面内容，否则返回0")]),t._v("\nfamilyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Family_Single'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" familyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'FamilySize'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfamilyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Family_Small'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" familyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'FamilySize'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfamilyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Family_Large'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" familyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'FamilySize'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" s "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),t._v("\nfull "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("familyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#到现在我们已经有了这么多个特征了")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(full.shape)")]),t._v("\n\n\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#3.3 特征选择")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 相关系数法：计算各个特征的相关系数")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#相关性矩阵")]),t._v("\ncorrDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(corrDf)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n查看各个特征与生成情况（Survived）的相关系数，\nascending=False表示按降序排列\n'''")]),t._v("\ncorrDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Survived'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort_values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ascending "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(corrDf['Survived'].sort_values(ascending =False))")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据各个特征与生成情况（Survived）的相关系数大小，我们选择了这几个特征作为模型的输入：")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 头衔（前面所在的数据集titleDf）、")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 客舱等级（pclassDf）、家庭大小（familyDf）、船票价格（Fare）、")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 船舱号（cabinDf）、登船港口（embarkedDf）、性别（Sex）")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#特征选择")]),t._v("\nfull_X "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("titleDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#头衔")]),t._v("\n                     pclassDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#客舱等级")]),t._v("\n                     familyDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#家庭大小")]),t._v("\n                     full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Fare'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#船票价格")]),t._v("\n                     cabinDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#船舱号")]),t._v("\n                     embarkedDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#登船港口")]),t._v("\n                     full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sex'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#性别")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.构建模型")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用训练数据和某个机器学习算法得到机器学习模型，用测试数据评估模型")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.1 建立训练数据集和测试数据集")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n1）坦尼克号测试数据集因为是我们最后要提交给Kaggle的，里面没有生存情况的值，所以不能用于评估模型。\n我们将Kaggle泰坦尼克号项目给我们的测试数据，叫做预测数据集（记为pred,也就是预测英文单词predict的缩写）。\n也就是我们使用机器学习模型来对其生存情况就那些预测。\n2）我们使用Kaggle泰坦尼克号项目给的训练数据集，做为我们的原始数据集（记为source），\n从这个原始数据集中拆分出训练数据集（记为train：用于模型训练）和测试数据集（记为test：用于模型评估）。\n\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#原始数据集有891行")]),t._v("\nsourceRow"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("891")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nsourceRow是我们在最开始合并数据前知道的，原始数据集有总共有891条数据\n从特征集合full_X中提取原始数据集提取前891行数据时，我们要减去1，因为行号是从0开始的。\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#原始数据集：特征")]),t._v("\nsource_X "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("sourceRow"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#原始数据集：标签")]),t._v("\nsource_y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("sourceRow"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Survived'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(full_X.columns)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测数据集：特征")]),t._v("\npred_X "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sourceRow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n上面代码解释：\n891行前面的数据是测试数据集，891行之后的数据是预测数据集。[sourceRow:,:]就是从891行开始到最后一行作为预测数据集\n'''")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n确保这里原始数据集取的是前891行的数据，不然后面模型会有错误\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#原始数据集有多少行")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据集有多少行:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("source_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#预测数据集大小")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据集有多少行:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("pred_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n从原始数据集（source）中拆分出训练数据集（用于模型训练train），测试数据集（用于模型评估test）\ntrain_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train data和test data\ntrain_data：所要划分的样本特征集\ntrain_target：所要划分的样本结果\ntest_size：样本占比，如果是整数的话就是样本的数量\n'''")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split \n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#建立模型用的训练数据集和测试数据集")]),t._v("\ntrain_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                    source_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                    train_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v(".8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#输出数据集大小")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据集特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("source_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'训练数据集特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("train_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'测试数据集特征：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("test_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'原始数据集标签：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("source_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n       "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'训练数据集标签：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("train_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'测试数据集标签：'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("test_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.2 选择机器学习算法")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 选择一个机器学习算法，用于模型的训练。如果你是新手，建议从逻辑回归算法开始")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第1步：导入算法")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LogisticRegression\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第2步：创建模型：逻辑回归（logisic regression）")]),t._v("\nmodel "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LogisticRegression"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其他算法")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#随机森林Random Forests Model")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#from sklearn.ensemble import RandomForestClassifier")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#model = RandomForestClassifier(n_estimators=100)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#支持向量机Support Vector Machines")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#from sklearn.svm import SVC, LinearSVC")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#model = SVC()")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Gradient Boosting Classifier")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#from sklearn.ensemble import GradientBoostingClassifier")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#model = GradientBoostingClassifier()")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#K-nearest neighbors")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#from sklearn.neighbors import KNeighborsClassifier")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#model = KNeighborsClassifier(n_neighbors = 3)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Gaussian Naive Bayes")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#from sklearn.naive_bayes import GaussianNB")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#model = GaussianNB()")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.3 训练模型")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第3步：训练模型")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" train_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分类问题，score得到的是模型的正确率")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_X "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6.方案实施（Deployment）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6.1 得到预测结果上传到Kaggle")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用预测数据集到底预测结果，并保存到csv文件中，上传到Kaggle中，就可以看到排名。")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#使用机器学习模型，对预测数据集中的生存情况进行预测")]),t._v("\npred_Y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n生成的预测值是浮点数（0.0,1,0）\n但是Kaggle要求提交的结果是整型（0,1）\n所以要对数据类型进行转换\n'''")]),t._v("\npred_Y"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pred_Y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#乘客id")]),t._v("\npassenger_id "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" full"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sourceRow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'PassengerId'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#数据框：乘客id，预测生存情况的值")]),t._v("\npredDf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" \n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'PassengerId'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" passenger_id "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n     "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Survived'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pred_Y "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npredDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\npredDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#保存结果")]),t._v("\npredDf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'titanic_pred.csv'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"泰坦尼克号案例的补充"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#泰坦尼克号案例的补充"}},[t._v("#")]),t._v(" 泰坦尼克号案例的补充")]),t._v(" "),n("p",[t._v("size\n用来计算数组和矩阵中所有元素的个数")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("a = np.array([[1, 2, 3], [4, 5, 6]])\nnp.size(a)    # 返回值为6\nnp.size(a,1)    # 返回值为3\n\nshape\n用来计算矩阵每维的大小\nnp.shape(a)    # 返回值为(2, 3)\n\nfull.describe()  \n# 从数据总数，可以看出那个标签存在缺失值\n\n# 船票最小值为0，说明有缺失值，清洗处理的时候，可以用平均值补充\n#describe只能对数据类型的数据进行描述，对其他类型的数据则无用了，需要其他方法来协助 full.info()\n\nfull.info() #查看数据类型和总数\n\ndescribe和info就让我明了要朝什么方向清洗数据了\n\n")])])]),n("p",[t._v("为什么训练数据比测试数据多一个维度，就是多个比如泰坦尼克案例中，训练多测试数据，幸存结果的数据。我们建立模型就是为了预测这个值，所以测试数据中没有。")]),t._v(" "),n("p",[t._v("数据清洗包括  数据预处理和特征工程\n步骤")]),t._v(" "),n("ol",[n("li",[t._v("选择子集")]),t._v(" "),n("li",[t._v("列明重命名（当列明不符合习惯，或者不方面数据分析时）")]),t._v(" "),n("li",[t._v("缺失数据处理（）")]),t._v(" "),n("li",[t._v("数据类型转换（如文本‘10’转数字10）")]),t._v(" "),n("li",[t._v("数据排序")]),t._v(" "),n("li",[t._v("异常值处理")])]),t._v(" "),n("h2",{attrs:{id:"结合taitan-test-py看"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#结合taitan-test-py看"}},[t._v("#")]),t._v(" 结合taitan-test.py看")])])}),[],!1,null,null,null);s.default=e.exports}}]);