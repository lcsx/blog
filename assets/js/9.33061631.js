(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{281:function(t,s,a){t.exports=a.p+"assets/img/python1.3a965c75.png"},282:function(t,s,a){t.exports=a.p+"assets/img/pandas1.238160f2.png"},283:function(t,s){t.exports="data:image/png;base64,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"},351:function(t,s,a){"use strict";a.r(s);var n=a(28),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"csv"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#csv"}},[t._v("#")]),t._v(" CSV")]),t._v(" "),n("p",[t._v("CSV（comma-separated value，逗号分隔值）文件格式是一种非常简单的数据存储与分享\n方式。CSV 文件将数据表格存储为纯文本，表格（或电子表格）中的每个单元格都是一个\n数值或字符串。与 Excel 文件相比，CSV 文件的一个主要优点是有很多程序可以存储、转\n换和处理纯文本文件；相比之下，能够处理 Excel 文件的程序却不多。所有电子表格程序、\n文字处理程序或简单的文本编辑器都可以处理纯文本文件，但不是所有的程序都能处理\nExcel 文件。尽管 Excel 是一个功能非常强大的工具，但是当你使用 Excel 文件时，还是会\n被局限在 Excel 提供的功能范围内。")]),t._v(" "),n("p",[t._v("当你使用 CSV 文件时，确实会失去某些 Excel 功能：在 Excel 电子表格中，每个单元格都\n有一个定义好的“类型”（数值、文本、货币、日期等），CSV 文件中的单元格则只是原\n始数据。幸好，Python 在识别不同数据类型方面相当聪明，第 1 章中已经展示了这一点。\n使用 CSV 文件的另一个问题是它只能保存数据，不能保存公式。但是，通过将数据存储\n（CSV 文件）和数据处理（Python 脚本）分离，你可以很容易地在不同数据集上进行加工\n处理。当数据存储和数据处理过程分开进行时，错误（不管是数据处理中的错误，还是数\n据存储中的错误）不但更容易被发现，而且更难扩散。")]),t._v(" "),n("p",[n("strong",[t._v("csv只有一个sheet表，与excel不同")])]),t._v(" "),n("p"),n("div",{staticClass:"table-of-contents"},[n("ul",[n("li",[n("a",{attrs:{href:"#csv"}},[t._v("CSV")]),n("ul",[n("li",[n("a",{attrs:{href:"#文件打开模式"}},[t._v("文件打开模式")])]),n("li",[n("a",{attrs:{href:"#不使用csv模块"}},[t._v("不使用csv模块")])]),n("li",[n("a",{attrs:{href:"#筛选特定的行"}},[t._v("筛选特定的行")]),n("ul",[n("li",[n("a",{attrs:{href:"#行中的值满足某个条件"}},[t._v("行中的值满足某个条件")])]),n("li",[n("a",{attrs:{href:"#行中的值属于某个集合"}},[t._v("行中的值属于某个集合")])]),n("li",[n("a",{attrs:{href:"#行中的值匹配于某个模式（正则表达式）"}},[t._v("行中的值匹配于某个模式（正则表达式）")]),n("ul",[n("li",[n("a",{attrs:{href:"#next"}},[t._v("next()")])])])]),n("li",[n("a",{attrs:{href:"#pandas的loc"}},[t._v("pandas的loc")])]),n("li",[n("a",{attrs:{href:"#pandas取行列数据的更多操作"}},[t._v("pandas取行列数据的更多操作")]),n("ul",[n("li",[n("a",{attrs:{href:"#iloc-函数来根据索引位置选取列"}},[t._v("iloc 函数来根据索引位置选取列")])]),n("li",[n("a",{attrs:{href:"#两种列索引方式"}},[t._v("两种列索引方式")])])])]),n("li",[n("a",{attrs:{href:"#丢弃行数据"}},[t._v("丢弃行数据")])]),n("li",[n("a",{attrs:{href:"#添加标题"}},[t._v("添加标题")]),n("ul",[n("li",[n("a",{attrs:{href:"#pandas方式"}},[t._v("pandas方式")])])])]),n("li",[n("a",{attrs:{href:"#读取多个cvs"}},[t._v("读取多个cvs")]),n("ul",[n("li",[n("a",{attrs:{href:"#遍历多个文件，将平均值输出单独文件"}},[t._v("遍历多个文件，将平均值输出单独文件")])]),n("li",[n("a",{attrs:{href:"#pandas方式"}},[t._v("pandas方式")])])])])])])])])])]),n("p"),t._v(" "),n("h2",{attrs:{id:"文件打开模式"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#文件打开模式"}},[t._v("#")]),t._v(" 文件打开模式")]),t._v(" "),n("p",[n("img",{attrs:{src:a(281),alt:""}})]),t._v(" "),n("h2",{attrs:{id:"不使用csv模块"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#不使用csv模块"}},[t._v("#")]),t._v(" 不使用csv模块")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python3")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n\ninput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\data\\data\\aa.csv'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\a.txt'")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        header "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("readline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      \n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        header "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" header"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        header_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" header"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("header_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filewriter.write(是str字符串而不是DataFrame)")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            row "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            row_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("row_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("strong",[t._v("filereader.readline() 上面文件被读过第一行后，下面语句for row in filereader里就不包括读过的那行了")]),t._v(" "),n("strong",[t._v("with open(output_file, 'w', newline='') as filewriter ，then filewriter.write(是直接写入的是str字符串)")]),t._v(" "),n("strong",[t._v("filewriter = csv.writer(csv_out_file)，  filewriter.writerow(这是是DataFrame)")])]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",{staticStyle:{"text-align":"center"}},[t._v("函数")]),t._v(" "),n("th",{staticStyle:{"text-align":"center"}},[t._v("解释")]),t._v(" "),n("th",{staticStyle:{"text-align":"center"}})])]),t._v(" "),n("tbody",[n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("fileObject.readline(size)")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v('用于从文件读取整行，如果指定了一个非负数的参数，则返回指定大小的字节数，包括 "\\n" 字符')]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("readline()读取第一行，")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("str.join(sequence)")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("将序列中的元素以指定的字符连接生成一个新的字符串")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("','.join(map(str,header_list)) -- header_list序列转str后用,连结")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("map(function, iterable, ...)")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("会根据提供的函数对指定序列做映射")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("map(lambda x: x ** 2, [1, 2, 3, 4, 5])")])])])]),t._v(" "),n("h2",{attrs:{id:"筛选特定的行"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#筛选特定的行"}},[t._v("#")]),t._v(" 筛选特定的行")]),t._v(" "),n("h3",{attrs:{id:"行中的值满足某个条件"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#行中的值满足某个条件"}},[t._v("#")]),t._v(" 行中的值满足某个条件")]),t._v(" "),n("h3",{attrs:{id:"行中的值属于某个集合"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#行中的值属于某个集合"}},[t._v("#")]),t._v(" 行中的值属于某个集合")]),t._v(" "),n("h3",{attrs:{id:"行中的值匹配于某个模式（正则表达式）"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#行中的值匹配于某个模式（正则表达式）"}},[t._v("#")]),t._v(" 行中的值匹配于某个模式（正则表达式）")]),t._v(" "),n("h4",{attrs:{id:"next"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#next"}},[t._v("#")]),t._v(" next()")]),t._v(" "),n("p",[t._v("next() 返回迭代器的下一个项目,next() 函数要和生成迭代器的iter() 函数一起使用,这里估计是有了iter了吧\n同样，next()后，后面的文件读取就不包括刚才的那行数据了")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python3")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" csv\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ninput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\data\\data\\aa.csv'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\a.txt'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        filereader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        filewriter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        header "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# next() 返回迭代器的下一个项目,next() 函数要和生成迭代器的iter() 函数一起使用,这里估计是有了iter了吧")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 同样，next()后，后面的文件读取就不包括刚才的那行数据了")]),t._v("\n        \n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row_list "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            supplier "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("supplier"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     cost = str(row_list[3]).strip('$').replace(',', '')")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     if supplier == 'Supplier Z' or float(cost) > 600.0:")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         filewriter.writerow(row_list)")]),t._v("\n")])])]),n("h3",{attrs:{id:"pandas的loc"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#pandas的loc"}},[t._v("#")]),t._v(" pandas的loc")]),t._v(" "),n("p",[t._v("pandas 提供了一个 loc 函数，可以同时选择特定的行与列。你需要在逗号前面设定行筛选条件，在逗号后面设定列筛选条件。")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("列label1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取所有行，列名称为label1")]),t._v("\npd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("列label1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("列label2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \npd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("行条件"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# :表示所有")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# #!/usr/bin/env python3")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import csv")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import sys")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input_file = r'C:\\Users\\46957\\Desktop\\pytest\\data\\data\\aa.csv'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output_file = r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\a.txt'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# with open(input_file, 'r', newline='') as csv_in_file:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     with open(output_file, 'w', newline='') as csv_out_file:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         filereader = csv.reader(csv_in_file)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         filewriter = csv.writer(csv_out_file)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         header = next(filereader)        # next() 返回迭代器的下一个项目,next() 函数要和生成迭代器的iter() 函数一起使用,这里估计是有了iter了吧")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         # 同样，next()后，后面的文件读取就不包括刚才的那行数据了")]),t._v("\n        \n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         for row_list in filereader:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#             supplier = str(row_list[0]).strip()")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#             print(supplier)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#             cost = str(row_list[3]).strip('$').replace(',', '')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         #     if supplier == 'Supplier Z' or float(cost) > 600.0:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#         #         filewriter.writerow(row_list)                          # filewriter.writerow(这是是DataFrame)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python3")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ninput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\data\\data\\aa.csv'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\bb.csv'")]),t._v("\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'$'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(data_frame['satisfaction_level'])")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pandas 提供了一个 loc 函数，可以同时选择特定的行与列。你需要在逗号前面设定行筛选条件，在逗号后面设定列筛选条件。")]),t._v("\n\ndata_frame_value_meets_condition "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contains"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'low'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.11")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# data_frame_value_meets_condition.to_csv(output_file, index=False)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(data_frame_value_meets_condition)")]),t._v("\n\ndata_frame_value_meets_condition222 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"department"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ndata_frame_value_meets_condition222 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"department"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"satisfaction_level"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame_value_meets_condition222"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame_value_meets_condition222"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"pandas取行列数据的更多操作"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#pandas取行列数据的更多操作"}},[t._v("#")]),t._v(" pandas取行列数据的更多操作")]),t._v(" "),n("p",[n("strong",[t._v("header=None决定0开始在哪里")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("data_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("img",{attrs:{src:a(282),alt:""}})]),t._v(" "),n("h4",{attrs:{id:"iloc-函数来根据索引位置选取列"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#iloc-函数来根据索引位置选取列"}},[t._v("#")]),t._v(" iloc 函数来根据索引位置选取列")]),t._v(" "),n("h4",{attrs:{id:"两种列索引方式"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#两种列索引方式"}},[t._v("#")]),t._v(" 两种列索引方式")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" csv\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ninput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\data\\data\\aa.csv'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\bb.csv'")]),t._v("\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'$'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 列索引")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 按索引取第一，第四列数据")]),t._v("\nmy_columns "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        filereader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        filewriter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row_list "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            row_list_output "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index_value "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" my_columns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                row_list_output"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index_value"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row_list_output"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dataFrame.to_csv(输出地址)")]),t._v("\n")])])]),n("p",[n("strong",[t._v("dataFrame.to_csv(输出地址)")]),t._v(" "),n("strong",[t._v("csv_out_file可以直接write(str)")]),t._v(" "),n("strong",[t._v("writerow(dataFrame字符串序列)")])]),t._v(" "),n("h3",{attrs:{id:"丢弃行数据"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#丢弃行数据"}},[t._v("#")]),t._v(" 丢弃行数据")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#（也就是行索引为 0，1，2 的那些行）")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndata_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reindex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删掉了0，1，4行，重新计算第一行的index，从原来的index为2开始")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(data_frame)")]),t._v("\n\ndata_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"添加标题"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#添加标题"}},[t._v("#")]),t._v(" 添加标题")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加标题")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        filereader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        filewriter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        header_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Supplier Name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Invoice Number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Part Number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cost'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Purchase Date'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\t\n")])])]),n("h4",{attrs:{id:"pandas方式"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#pandas方式"}},[t._v("#")]),t._v(" pandas方式")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("header_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Supplier Name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Invoice Number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n"),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Part Number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Cost'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Purchase Date'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ndata_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" names"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("header_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("strong",[t._v("data_frame = pd.read_csv(input_file, header=None, names=header_list)")])]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),n("p",[t._v("header 决定第一行从哪里开始，header=None 就是列上无标签的那种，直接第一行就是数据\nnames  标题")])]),t._v(" "),n("h3",{attrs:{id:"读取多个cvs"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#读取多个cvs"}},[t._v("#")]),t._v(" 读取多个cvs")]),t._v(" "),n("p",[t._v("两种方式，将多个输入文件中的数据垂直连接成一个输出文件")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" csv\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 读取多个csv文件，输出到一个文件中")]),t._v("\n\ninput_dir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\mulu\\bianliduogecvs'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\bb.csv'")]),t._v("\n\nfilelist "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_dir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"*.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(filelist)")]),t._v("\nfile_count "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 方式1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过csv.writer(csv_out_file)，就可以writerow(list数据)")]),t._v("\nfirst_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filelist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    file_count "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            filereader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            filewriter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" first_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#['$0.38 ', '0.53', '2', '157', '3', '0', '1', '0', 'sales', 'low']")]),t._v("\n                    filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                first_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                header "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pandas方式")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dataframe.to_csv()")]),t._v("\nall_data_frames "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filelist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index_col"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    all_data_frames"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame_concat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("all_data_frames"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ignore_index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frame_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# to_csv可不管你open的mode模式哦，直接是覆盖，所以先将数据append，再统一concat，然后输出")]),t._v("\n\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),n("p",[n("strong",[t._v("dataframe.append(dataframe)再统一concat,最后dataFrame.to_csv()")])]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),n("p",[t._v("这段代码垂直堆叠数据框。如果你需要平行连接数据，那么就在 concat 函数中设置axis=1。\n除了数据框，pandas 中还有一个数据容器，称为序列。你可以使用同样的语法去连接序列，只是要将连接的对象由数据框（是dataFrame？）改为序列。")])]),t._v(" "),n("h4",{attrs:{id:"遍历多个文件，将平均值输出单独文件"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#遍历多个文件，将平均值输出单独文件"}},[t._v("#")]),t._v(" 遍历多个文件，将平均值输出单独文件")]),t._v(" "),n("h5",{attrs:{id:"非pandas"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#非pandas"}},[t._v("#")]),t._v(" 非pandas")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" csv\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ninput_dir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\mulu\\bianliduogecvs'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\lcs.csv'")]),t._v("\noutput_header_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'file_name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'total_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'average_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncsv_out_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfilewriter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfilewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_header_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" input_file "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_dir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"*.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newline"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        filereader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_in_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        output_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("basename"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    \n        header "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        total_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\n        number_of_sales "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        average_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filereader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            total_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("satisfaction_level"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'$'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            number_of_sales "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        average_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{0:.2f}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" number_of_sales"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total_satisfaction_level"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("average_satisfaction_level"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        filewriter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writerow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncsv_out_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("strong",[t._v("os.path.basename(input_file)获取文件名，不包含后缀")])]),t._v(" "),n("p",[n("img",{attrs:{src:a(283),alt:""}})]),t._v(" "),n("h4",{attrs:{id:"pandas方式-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#pandas方式-2"}},[t._v("#")]),t._v(" pandas方式")]),t._v(" "),n("p",[t._v("pandas 提供了可以用来计算行和列统计量的摘要统计函数，比如 sum 和 mean。")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ninput_dir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\mulu\\bianliduogecvs'")]),t._v("\noutput_file "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'C:\\Users\\46957\\Desktop\\pytest\\basisofpython\\lcs1.csv'")]),t._v("\nall_files "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_dir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"*.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nall_data_frames "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" input_file "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" all_files"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_frame "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index_col"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    total_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("value"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'$'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" value "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    average_satisfaction_level "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("value"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'$'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" value "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" data_frame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'file_name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("basename"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'total_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" total_satisfaction_level"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n\t"),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'average_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" average_satisfaction_level"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t\n    all_data_frames"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'file_name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'total_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \\\n\t"),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'average_satisfaction_level'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 序列list传到dataFrame")]),t._v("\n\ndata_frames_concat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("all_data_frames"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ignore_index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_frames_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_file"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),n("p",[t._v('获取dataFrame里，某列，所有行(有":")的和')]),t._v(" "),n("p",[t._v("pd.DataFrame([float(str(value).strip('$').replace(',','')) for value in data_frame.loc[:, 'satisfaction_level']]).sum()")]),t._v(" "),n("p",[t._v("将序列list，如一个object传到dataframe")]),t._v(" "),n("p",[t._v('通过pd.DataFrame(object,columns=["列标题1","列标题2"]👇👇👇👇👇👇')]),t._v(" "),n("p",[t._v("data = {'file_name': os.path.basename(input_file),'total_satisfaction_level': total_satisfaction_level}")]),t._v(" "),n("p",[t._v("使用数据框函数将这个对象转换为一个 DataFrame👇👇👇👇👇")]),t._v(" "),n("p",[t._v("data_frames.append(pd.DataFrame(data, columns=['file_name', 'total_satisfaction_level']))")]),t._v(" "),n("p",[t._v("使用 concat 函数将这些数据框连接成为一个数据框，然后将这个数据框写入输出文件")])])])}),[],!1,null,null,null);s.default=e.exports}}]);